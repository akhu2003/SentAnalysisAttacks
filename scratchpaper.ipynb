{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/aaronhu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6696\n"
     ]
    }
   ],
   "source": [
    "text = \"I love BTC as a gambling habit!\"\n",
    "\n",
    "scores = analyzer.polarity_scores(text)\n",
    "\n",
    "print(scores['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 378/378 [00:14<00:00, 25.36it/s]\n"
     ]
    }
   ],
   "source": [
    "date_counts = [len(df[df['Date'] == d]) for d in tqdm(df['Date'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[581,\n",
       " 687,\n",
       " 539,\n",
       " 292,\n",
       " 237,\n",
       " 420,\n",
       " 641,\n",
       " 675,\n",
       " 478,\n",
       " 405,\n",
       " 522,\n",
       " 312,\n",
       " 418,\n",
       " 402,\n",
       " 405,\n",
       " 170,\n",
       " 287,\n",
       " 336,\n",
       " 260,\n",
       " 293,\n",
       " 281,\n",
       " 321,\n",
       " 159,\n",
       " 208,\n",
       " 210,\n",
       " 378,\n",
       " 321,\n",
       " 312,\n",
       " 245,\n",
       " 216,\n",
       " 349,\n",
       " 342,\n",
       " 457,\n",
       " 365,\n",
       " 391,\n",
       " 461,\n",
       " 512,\n",
       " 515,\n",
       " 459,\n",
       " 551,\n",
       " 534,\n",
       " 557,\n",
       " 566,\n",
       " 539,\n",
       " 482,\n",
       " 592,\n",
       " 523,\n",
       " 558,\n",
       " 37261,\n",
       " 75165,\n",
       " 694,\n",
       " 560,\n",
       " 574,\n",
       " 551,\n",
       " 487,\n",
       " 587,\n",
       " 568,\n",
       " 668,\n",
       " 878,\n",
       " 500,\n",
       " 610,\n",
       " 572,\n",
       " 725,\n",
       " 1055,\n",
       " 1078,\n",
       " 85196,\n",
       " 72384,\n",
       " 781,\n",
       " 807,\n",
       " 801,\n",
       " 832,\n",
       " 680,\n",
       " 883,\n",
       " 795,\n",
       " 795,\n",
       " 769,\n",
       " 779,\n",
       " 801,\n",
       " 741,\n",
       " 759,\n",
       " 901,\n",
       " 803,\n",
       " 894,\n",
       " 844,\n",
       " 836,\n",
       " 909,\n",
       " 717,\n",
       " 967,\n",
       " 1186,\n",
       " 872,\n",
       " 891,\n",
       " 855,\n",
       " 846,\n",
       " 810,\n",
       " 756,\n",
       " 759,\n",
       " 847,\n",
       " 715,\n",
       " 737,\n",
       " 11191,\n",
       " 87208,\n",
       " 884,\n",
       " 710,\n",
       " 787,\n",
       " 735,\n",
       " 830,\n",
       " 717,\n",
       " 737,\n",
       " 866,\n",
       " 790,\n",
       " 839,\n",
       " 849,\n",
       " 746,\n",
       " 695,\n",
       " 676,\n",
       " 788,\n",
       " 818,\n",
       " 728,\n",
       " 842,\n",
       " 863,\n",
       " 770,\n",
       " 741,\n",
       " 694,\n",
       " 821,\n",
       " 777,\n",
       " 857,\n",
       " 848,\n",
       " 843,\n",
       " 863,\n",
       " 763,\n",
       " 802,\n",
       " 793,\n",
       " 725,\n",
       " 785,\n",
       " 779,\n",
       " 725,\n",
       " 701,\n",
       " 781,\n",
       " 44804,\n",
       " 35452,\n",
       " 763,\n",
       " 744,\n",
       " 817,\n",
       " 811,\n",
       " 849,\n",
       " 744,\n",
       " 686,\n",
       " 728,\n",
       " 720,\n",
       " 731,\n",
       " 732,\n",
       " 835,\n",
       " 724,\n",
       " 723,\n",
       " 717,\n",
       " 807,\n",
       " 807,\n",
       " 684,\n",
       " 797,\n",
       " 789,\n",
       " 712,\n",
       " 737,\n",
       " 761,\n",
       " 687,\n",
       " 920,\n",
       " 863,\n",
       " 751,\n",
       " 712,\n",
       " 781,\n",
       " 782,\n",
       " 757,\n",
       " 734,\n",
       " 647,\n",
       " 688,\n",
       " 681,\n",
       " 776,\n",
       " 705,\n",
       " 796,\n",
       " 628,\n",
       " 619,\n",
       " 791,\n",
       " 767,\n",
       " 725,\n",
       " 814,\n",
       " 845,\n",
       " 625,\n",
       " 503,\n",
       " 538,\n",
       " 537,\n",
       " 638,\n",
       " 683,\n",
       " 607,\n",
       " 692,\n",
       " 607,\n",
       " 27960,\n",
       " 621,\n",
       " 645,\n",
       " 585,\n",
       " 582,\n",
       " 595,\n",
       " 573,\n",
       " 679,\n",
       " 618,\n",
       " 725,\n",
       " 553,\n",
       " 551,\n",
       " 581,\n",
       " 720,\n",
       " 735,\n",
       " 703,\n",
       " 771,\n",
       " 755,\n",
       " 617,\n",
       " 652,\n",
       " 729,\n",
       " 681,\n",
       " 697,\n",
       " 686,\n",
       " 739,\n",
       " 737,\n",
       " 792,\n",
       " 886,\n",
       " 809,\n",
       " 745,\n",
       " 700,\n",
       " 661,\n",
       " 627,\n",
       " 686,\n",
       " 673,\n",
       " 741,\n",
       " 684,\n",
       " 851,\n",
       " 685,\n",
       " 715,\n",
       " 668,\n",
       " 753,\n",
       " 849,\n",
       " 830,\n",
       " 664,\n",
       " 723,\n",
       " 651,\n",
       " 676,\n",
       " 764,\n",
       " 794,\n",
       " 774,\n",
       " 763,\n",
       " 757,\n",
       " 670,\n",
       " 656,\n",
       " 733,\n",
       " 30500,\n",
       " 38608,\n",
       " 35846,\n",
       " 8576,\n",
       " 682,\n",
       " 600,\n",
       " 575,\n",
       " 643,\n",
       " 696,\n",
       " 565,\n",
       " 629,\n",
       " 583,\n",
       " 537,\n",
       " 732,\n",
       " 706,\n",
       " 632,\n",
       " 723,\n",
       " 644,\n",
       " 656,\n",
       " 728,\n",
       " 706,\n",
       " 32099,\n",
       " 581,\n",
       " 565,\n",
       " 589,\n",
       " 717,\n",
       " 638,\n",
       " 603,\n",
       " 617,\n",
       " 595,\n",
       " 526,\n",
       " 566,\n",
       " 513,\n",
       " 645,\n",
       " 609,\n",
       " 562,\n",
       " 626,\n",
       " 28016,\n",
       " 23572,\n",
       " 20577,\n",
       " 611,\n",
       " 687,\n",
       " 649,\n",
       " 577,\n",
       " 618,\n",
       " 544,\n",
       " 709,\n",
       " 705,\n",
       " 605,\n",
       " 627,\n",
       " 588,\n",
       " 541,\n",
       " 571,\n",
       " 605,\n",
       " 639,\n",
       " 603,\n",
       " 562,\n",
       " 494,\n",
       " 642,\n",
       " 538,\n",
       " 587,\n",
       " 660,\n",
       " 611,\n",
       " 663,\n",
       " 595,\n",
       " 598,\n",
       " 631,\n",
       " 631,\n",
       " 2495,\n",
       " 556,\n",
       " 608,\n",
       " 679,\n",
       " 819,\n",
       " 780,\n",
       " 654,\n",
       " 769,\n",
       " 640,\n",
       " 609,\n",
       " 792,\n",
       " 714,\n",
       " 728,\n",
       " 737,\n",
       " 679,\n",
       " 662,\n",
       " 666,\n",
       " 829,\n",
       " 719,\n",
       " 687,\n",
       " 726,\n",
       " 737,\n",
       " 785,\n",
       " 859,\n",
       " 716,\n",
       " 785,\n",
       " 740,\n",
       " 775,\n",
       " 772,\n",
       " 701,\n",
       " 670,\n",
       " 832,\n",
       " 853,\n",
       " 794,\n",
       " 849,\n",
       " 950,\n",
       " 814,\n",
       " 841,\n",
       " 982,\n",
       " 1054,\n",
       " 1275,\n",
       " 1226,\n",
       " 36139,\n",
       " 63301,\n",
       " 54192,\n",
       " 78112,\n",
       " 74705,\n",
       " 64890,\n",
       " 58586,\n",
       " 57678,\n",
       " 43284,\n",
       " 51355,\n",
       " 40887,\n",
       " 60145,\n",
       " 56770,\n",
       " 52814,\n",
       " 54946,\n",
       " 43835,\n",
       " 48524,\n",
       " 52962]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df['Date'].unique()\n",
    "dates.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative', 'Positive', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence[5]: \"Flair makes this trivial.\"'/'NEGATIVE' (0.8851)]\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('sentiment')\n",
    "s = Sentence(\"Flair makes this trivial.\")\n",
    "classifier.predict(s)\n",
    "print(s.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence[3]: \"I hate you\"'/'NEGATIVE' (0.9997)]\n",
      "['Sentence[4]: \"You make me angry\"'/'NEGATIVE' (0.9792)]\n",
      "['Sentence[7]: \"You are the love of my life\"'/'POSITIVE' (0.9953)]\n",
      "['Sentence[1]: \"Meow\"'/'POSITIVE' (0.9915)]\n",
      "['Sentence[3]: \"Whats up friend\"'/'POSITIVE' (0.9992)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_tweets = ['I hate you','You make me angry','You are the love of my life','Meow','Whats up friend']\n",
    "for t in test_tweets:\n",
    "    s = Sentence(t)\n",
    "    classifier.predict(s)\n",
    "    print(s.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9950538873672485}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. Load a sentiment-analysis pipeline (defaults to a pretrained model)\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 2. Define your sentence\n",
    "sentence = \"Hugging Face makes NLP tasks super easy!\"\n",
    "\n",
    "# 3. Run the classifier\n",
    "result = classifier(sentence)\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Example output: [{'label': 'POSITIVE', 'score': 0.9998}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/aaronhu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from typing import List, Union\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "    \n",
    "def naive_analyzer(\n",
    "    tweets: Union[str, List[str]]\n",
    ") -> Union[float, List[float]]:\n",
    "    \"\"\"\n",
    "    If tweets is a single string, returns one compound score (float).\n",
    "    If tweets is a list of strings, returns a list of floats.\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Single‐string case\n",
    "    if isinstance(tweets, str):\n",
    "        return analyzer.polarity_scores(tweets)['compound']\n",
    "\n",
    "    # List‐of‐strings case\n",
    "    return [analyzer.polarity_scores(t)['compound'] for t in tweets]\n",
    "test_tweets = df.head(5)['tokenized no stop'].to_list()\n",
    "naive_analyzer(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '=', '$', '5018.57', 'mxn', '|', '$', '375.29', 'usd', 'precio', ':', '-', 'friday', '3rd', 'october', '2014', '12:00']\n",
      "['$', '373.88', ';', '$', '365.00', ';', 'instantly', 'buy', 'gh', '/', 'btc', ':']\n",
      "['live', ':', 'profit', '=', '$', '60.41', '(', '0.77', '%', ')', '.', 'buy', 'b21', '.', '08', '@', '$', '371.67', '(', ')', '.', 'sell', '@', '$', '375.00', '(', ')', '-']\n",
      "['last', '10', 'mins', ',', 'arb', 'opps', 'spanning', '33', 'exchange', 'pair', '(', ')', ',', 'yielding', 'profits', 'ranging', '$', '0.00', '$', '210.09']\n",
      "['live', ':', 'profit', '=', '$', '205.97', '(', '12.74', '%', ')', '.', 'buy', 'b4', '.', '38', '@', '$', '367.40', '(', ')', '.', 'sell', '@', '$', '375.00', '(', ')', '-']\n"
     ]
    }
   ],
   "source": [
    "for i in df.head(5)['tokenized no stop'].to_list():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Bitcoin’s fixed supply makes it digital gold f...\n",
       "1      Watching Bitcoin hit new all-time highs is pro...\n",
       "2      Every time I see Lightning Network transaction...\n",
       "3      Institutional adoption of Bitcoin shows real t...\n",
       "4      HODLing Bitcoin isn’t just an investment—it’s ...\n",
       "                             ...                        \n",
       "115    In times of volatility, Bitcoin’s non-correlat...\n",
       "116    Retail adoption surges as more wallets reach n...\n",
       "117    Bitcoin’s permissionless innovation invites an...\n",
       "118    Use Bitcoin in your business to cut transactio...\n",
       "119    Every satoshi you hold is a vote for a freer f...\n",
       "Name: create 30 positive tweets about bitcoin, Length: 120, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = pd.read_csv('Pos Adversarial GPT Tweets.csv')\n",
    "(ad.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Fake', 'score': 0.5081859230995178}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "detector = pipeline(\"text-classification\",\n",
    "                    model=\"roberta-base-openai-detector\")\n",
    "result = detector(\"Is this sentence human or AI?\")[0]\n",
    "print(result)  # e.g. {'label': 'Real', 'score': 0.80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fake'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Real', 'score': 0.5653204917907715}\n"
     ]
    }
   ],
   "source": [
    "detector = pipeline(\"text-classification\",\n",
    "                    model=\"roberta-base-openai-detector\")\n",
    "result = detector(\"tung tung sahoor\")[0]\n",
    "print(result)  # e.g. {'label': 'Real', 'score': 0.80}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
